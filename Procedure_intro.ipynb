{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run MGM to get direct or linear associations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XY_file_name = \"200p_1000samples_XY.txt\";\n",
    "data_folder_path = \"/ihome/hpark/zhf16/causalDeepVASE/data\";\n",
    "'''\n",
    "Run MGM\n",
    "Note: MGM was implemented in Java and the following Python APIs call the Java implementation.\n",
    "Please restart the Python program after encountering a JVM problem.\n",
    "The input data file should be \".txt\" format and should also include the response variables.\n",
    "Here is what the input data should look like:\n",
    "X1 X2 ... Xp Y1 ... Yq\n",
    "1  1  ... 1  1  ... 1\n",
    "'''\n",
    "# import the MGM package\n",
    "from MGM.MGM import MGM\n",
    "# Initialize a MGM object\n",
    "mgm = MGM();\n",
    "'''\n",
    "Run MGM\n",
    "Parameters:\n",
    "    data_folder_path: the directory at where the input data is located.\n",
    "    XY_file_name: the input data.\n",
    "    lambda_continuous_continuous: the panalty value 'lamda' set for the associations whose two variables are continuous.\n",
    "    lamda_continuous_discrete: the panalty value 'lamda' set for the associations whose one variable is continuous and the other is discrete.\n",
    "    lamda_discrete_discrete: the panalty value 'lamda' set for the associations whose two variables are discrete.\n",
    "    \n",
    "Return:\n",
    "    mgm_output_file: a file that contains all the selected associations.\n",
    "'''\n",
    "mgm_output_file = mgm.runMGM(data_folder_path, XY_file_name,lambda_continuous_continuous = 0.3, lamda_continuous_discrete = 0.3, lamda_discrete_discrete = 0.3);\n",
    "print(\"Please find MGM's output file as:\");\n",
    "mgm_output_file_path = data_folder_path+os.path.sep+mgm_output_file;\n",
    "print(mgm_output_file_path);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run DNN to get indirect or nonlinear associations\n",
    "##### Generate knockoff data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_file_name = \"200p_1000samples_X.csv\";\n",
    "'''\n",
    "#Generate knockoff data using one of three methods: ISEE Omega, DNN, and Cholesky_LU.\n",
    "#Recommended: ISEE Omega or Cholesky_LU.\n",
    "The code for generating ISEE Omega knockoff is implemented using R. Please make sure your computer has R installed.\n",
    "'''\n",
    "#Import the package\n",
    "from DL.knockoff.KnockoffGenerator import KnockoffGenerator;\n",
    "#Initialize the knockoff generator object\n",
    "generator = KnockoffGenerator();\n",
    "\n",
    "\n",
    "# knockoff_file_path = generator.Chol_Lu_knockoff(data_folder_path, X_file_name);\n",
    "\n",
    "#If want to generate ISEE Omega knockoff, please set the ISEE code path and R home environment.\n",
    "\n",
    "generator.set_ISEE_path(\"/ihome/hpark/zhf16/causalDeepVASE/\");\n",
    "\n",
    "generator.set_R_home('/ihome/hpark/zhf16/.conda/envs/env36/lib/R');\n",
    "\n",
    "knockoff_file_path = generator.ISEE_knockoff(data_folder_path, X_file_name);\n",
    "\n",
    "# Y_file_name = '200p_1000samples_Y.csv';\n",
    "\n",
    "# knockoff_file_path = generator.DNN_knockoff(data_folder_path, X_file_name,Y_file_name);\n",
    "\n",
    "print(\"The newly generated knockoff file is named as:\")\n",
    "print(knockoff_file_path);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''''''\n",
    "# After generating the knockoff data, run DNN\n",
    "Y_file_name = '200p_1000samples_Y.csv';\n",
    "X_knockoff_data = pd.read_csv(knockoff_file_path);\n",
    "print(X_knockoff_data.shape)\n",
    "# X_knockoff_data\n",
    "\n",
    "#nutrient_data\n",
    "original_data_Y = pd.read_csv(data_folder_path+os.path.sep+Y_file_name);\n",
    "# original_data_Y\n",
    "\n",
    "X_values = X_knockoff_data.values;\n",
    "Y_values = original_data_Y.values;\n",
    "    \n",
    "pVal = int(X_values.shape[1] / 2);\n",
    "n = X_values.shape[0];\n",
    "print(X_values.shape);\n",
    "print(Y_values.shape);\n",
    "print(pVal);\n",
    "    \n",
    "X_origin = X_values[:, 0:pVal];\n",
    "X_knockoff = X_values[:, pVal:];\n",
    "\n",
    "x3D_train = np.zeros((n, pVal, 2));\n",
    "x3D_train[:, :, 0] = X_origin;\n",
    "x3D_train[:, :, 1] = X_knockoff;\n",
    "label_train = Y_values;\n",
    "    \n",
    "coeff = 0.05 * np.sqrt(2.0 * np.log(pVal) / n);\n",
    "\n",
    "n_outputs = original_data_Y.shape[1];\n",
    "\n",
    "#Save the DNN output to the following directory.\n",
    "result_dir = 'data/DNN_result/';\n",
    "if not os.path.exists(result_dir):\n",
    "    os.makedirs(result_dir);\n",
    "    \n",
    "from DL.DNN.DNN import DNN;\n",
    "dnn = DNN();\n",
    "model = dnn.build_DNN(pVal, n_outputs, coeff);\n",
    "callback = DNN.Job_finish_Callback(result_dir,pVal);\n",
    "dnn.train_DNN(model, x3D_train, label_train,callback);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Apply FDR control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply FDR control to DNN result\n",
    "from DL.FDR.FDR_control import FDR_control;\n",
    "control = FDR_control();\n",
    "selected_features = control.controlFilter(data_folder_path +os.path.sep+ X_file_name, \"/ihome/hpark/zhf16/causalDeepVASE/data/DNN_result\", offset=1, q=0.05);\n",
    "#Save the selected associations\n",
    "selected_associations = [];\n",
    "for ele in selected_features:\n",
    "    selected_associations.append({\"Feature1\":ele,\"Feature2\":\"Y\"});\n",
    "pd.DataFrame(selected_associations).to_csv(\"data/DNN_selected_associations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run DG\n",
    "#Load data\n",
    "X_data = pd.read_csv(\"X_n1000_p50_rep20.csv\");\n",
    "# X_data\n",
    "Y_data = pd.read_csv('y_si_n1000_p50_rep20.csv');\n",
    "#Merge X and Y\n",
    "dataset = pd.concat([X_data, Y_data], axis=1, join='inner');\n",
    "print(dataset.shape);\n",
    "\n",
    "#Calculate the covariance matrix\n",
    "cov_mat = dataset.cov();\n",
    "corr_inv = np.linalg.inv(cov_mat)\n",
    "corr_inv = pd.DataFrame(data=corr_inv, index=cov_mat.index,columns=cov_mat.columns)\n",
    "# corr_inv.head(2)\n",
    "\n",
    "#Convert the columns to their numerical representations\n",
    "col_map = {};\n",
    "col_map_rev = {};\n",
    "col_list = dataset.columns.to_list();\n",
    "for index,ele in enumerate(col_list):\n",
    "    col_map[ele] = index;\n",
    "    col_map_rev[index] = ele;\n",
    "print(dataset.shape);\n",
    "\n",
    "#https://stats.stackexchange.com/questions/13810/threshold-for-correlation-coefficient-to-indicate-statistical-significance-of-a\n",
    "# t = dataset.shape[0]**(1/2)\n",
    "\n",
    "#The data may need to be normalized if neccessary.\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# scaler = MinMaxScaler();\n",
    "# scaled_values = scaler.fit_transform(dataset);\n",
    "# dataset.loc[:,:] = scaled_values;\n",
    "\n",
    "#Initialize DG object\n",
    "from causal.DegenerateGaussianScore import DegenerateGaussianScore\n",
    "dg = DegenerateGaussianScore(dataset,discrete_threshold=0.2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_associations_sum = [];\n",
    "#Load both MGM-identified and DNN associations\n",
    "MGM_associations = pd.read_csv(\"X_n1000_p50_rep20_MGM_associations.csv\");\n",
    "for index,row in MGM_associations.iterrows():\n",
    "    if row[\"Feature1\"]==\"Y\" or row[\"Feature2\"]==\"Y\":\n",
    "        print(\"Found.\");\n",
    "        selected_associations_sum.append({\"Feature1\":row[\"Feature1\"],\"Feature2\":row[\"Feature2\"]});\n",
    "        \n",
    "DNN_associations = pd.read_csv(\"DNN_selected_associations.csv\");\n",
    "for index,row in DNN_associations.iterrows():\n",
    "    selected_associations_sum.append({\"Feature1\":row[\"Feature1\"],\"Feature2\":row[\"Feature2\"]});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ele in selected_associations_sum:\n",
    "    f1 = ele[\"Feature1\"];\n",
    "    f2 = ele[\"Feature2\"];\n",
    "    \n",
    "    inv_val = abs(corr_inv[f1][f2]);\n",
    "    if inv_val<0.0:\n",
    "        continue;\n",
    "    \n",
    "    n1_idx = col_map[f1];\n",
    "    n2_idx = col_map[f2];\n",
    "    \n",
    "    s1 = dg.localScore(n1_idx,{n2_idx});\n",
    "    s2 = dg.localScore(n2_idx,{n1_idx});\n",
    "    \n",
    "    if s1<s2:\n",
    "        print(\"Cause: \"+f2+\", Effect: \"+f1);\n",
    "    elif s1>s2:\n",
    "        print(\"Cause: \"+f1+\", Effect: \"+f2);\n",
    "    else:\n",
    "        print(\"Same score.\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py365",
   "language": "python",
   "name": "py365"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ihome/hpark/zhf16/causalDeepVASE/MGM/tetradLite.jar\n",
      "Please find MGM's output file as:\n",
      "X_n1000_p50_rep20_MGM_associations.csv\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Run MGM\n",
    "Note: MGM was implemented in Java and the following Python APIs call the Java implementation.\n",
    "Please restart the Python program after encountering a JVM problem.\n",
    "The input data file should be \".txt\" format and should also include the response variables.\n",
    "'''\n",
    "from MGM.MGM import MGM\n",
    "mgm = MGM();\n",
    "mgm_output_file = mgm.runMGM( \"/ihome/hpark/zhf16/causalDeepVASE\", \"X_n1000_p50_rep20.txt\",lambda_continuous_continuous = 0.3, lamda_continuous_discrete = 0.3, lamda_discrete_discrete = 0.3);\n",
    "print(\"Please find MGM's output file as:\");\n",
    "print(mgm_output_file);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__init__\n",
      "(50, 50)\n",
      "(1000, 50)\n",
      "['K1', 'K2', 'K3', 'K4', 'K5', 'K6', 'K7', 'K8', 'K9', 'K10', 'K11', 'K12', 'K13', 'K14', 'K15', 'K16', 'K17', 'K18', 'K19', 'K20', 'K21', 'K22', 'K23', 'K24', 'K25', 'K26', 'K27', 'K28', 'K29', 'K30', 'K31', 'K32', 'K33', 'K34', 'K35', 'K36', 'K37', 'K38', 'K39', 'K40', 'K41', 'K42', 'K43', 'K44', 'K45', 'K46', 'K47', 'K48', 'K49', 'K50']\n",
      "The newly generated knockoff file is named as:\n",
      "/ihome/hpark/zhf16/causalDeepVASE/X_n1000_p50_rep20_chol_lu_knockoff.csv\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "#Generate knockoff data using one of three methods: Omega, DNN, Cholesky_LU\n",
    "#Recommended: Omega or Cholesky_LU\n",
    "'''\n",
    "from DL.knockoff.KnockoffGenerator import KnockoffGenerator;\n",
    "generator = KnockoffGenerator();\n",
    "file_path = generator.Chol_Lu_knockoff(\"/ihome/hpark/zhf16/causalDeepVASE\", \"X_n1000_p50_rep20.csv\");\n",
    "print(\"The newly generated knockoff file is named as:\")\n",
    "print(file_path);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 100)\n",
      "(1000, 1)\n",
      "50\n",
      "__init__parameters\n",
      "[layer]: Input\t[shape]: [None, 50, 2] \n",
      "\n",
      "[layer]: LocallyConnected1D\t[shape]: [None, 50, 1] \n",
      "\n",
      "[layer]: LocallyConnected1D\t[shape]: [None, 50, 1] \n",
      "\n",
      "[layer]: Flatten\t[shape]: [None, None] \n",
      "\n",
      "[layer]: Dense\t[shape]: [None, 50] \n",
      "\n",
      "[layer]: Dense\t[shape]: [None, 50] \n",
      "\n",
      "[layer]: Dense\t[shape]: [None, 1] \n",
      "\n",
      "Epoch 1/20\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 28744140.4287\n",
      "on_epoch_end\n",
      "h_local1_weight = (50, 2, 1)\n",
      "h_local2_weight = (50, 1, 1)\n",
      "h0 = (50, 2)\n",
      "h0_abs = (50, 2)\n",
      "h1 = (50, 50)\n",
      "h2 = (50, 50)\n",
      "h3 = (50, 1)\n",
      "W1 = (50, 50)\n",
      "W2 = (50, 50)\n",
      "W3 = (50, 1)\n",
      "Epoch 2/20\n",
      "1000/1000 [==============================] - 0s 225us/step - loss: 28742892.6100\n",
      "on_epoch_end\n",
      "h_local1_weight = (50, 2, 1)\n",
      "h_local2_weight = (50, 1, 1)\n",
      "h0 = (50, 2)\n",
      "h0_abs = (50, 2)\n",
      "h1 = (50, 50)\n",
      "h2 = (50, 50)\n",
      "h3 = (50, 1)\n",
      "W1 = (50, 50)\n",
      "W2 = (50, 50)\n",
      "W3 = (50, 1)\n",
      "Epoch 3/20\n",
      "1000/1000 [==============================] - 0s 180us/step - loss: 28735765.2700\n",
      "on_epoch_end\n",
      "h_local1_weight = (50, 2, 1)\n",
      "h_local2_weight = (50, 1, 1)\n",
      "h0 = (50, 2)\n",
      "h0_abs = (50, 2)\n",
      "h1 = (50, 50)\n",
      "h2 = (50, 50)\n",
      "h3 = (50, 1)\n",
      "W1 = (50, 50)\n",
      "W2 = (50, 50)\n",
      "W3 = (50, 1)\n",
      "Epoch 4/20\n",
      "1000/1000 [==============================] - 0s 158us/step - loss: 28701862.7225\n",
      "on_epoch_end\n",
      "h_local1_weight = (50, 2, 1)\n",
      "h_local2_weight = (50, 1, 1)\n",
      "h0 = (50, 2)\n",
      "h0_abs = (50, 2)\n",
      "h1 = (50, 50)\n",
      "h2 = (50, 50)\n",
      "h3 = (50, 1)\n",
      "W1 = (50, 50)\n",
      "W2 = (50, 50)\n",
      "W3 = (50, 1)\n",
      "Epoch 5/20\n",
      "1000/1000 [==============================] - 0s 160us/step - loss: 28587942.2175\n",
      "on_epoch_end\n",
      "h_local1_weight = (50, 2, 1)\n",
      "h_local2_weight = (50, 1, 1)\n",
      "h0 = (50, 2)\n",
      "h0_abs = (50, 2)\n",
      "h1 = (50, 50)\n",
      "h2 = (50, 50)\n",
      "h3 = (50, 1)\n",
      "W1 = (50, 50)\n",
      "W2 = (50, 50)\n",
      "W3 = (50, 1)\n",
      "Epoch 6/20\n",
      "1000/1000 [==============================] - 0s 163us/step - loss: 28259139.5950\n",
      "on_epoch_end\n",
      "h_local1_weight = (50, 2, 1)\n",
      "h_local2_weight = (50, 1, 1)\n",
      "h0 = (50, 2)\n",
      "h0_abs = (50, 2)\n",
      "h1 = (50, 50)\n",
      "h2 = (50, 50)\n",
      "h3 = (50, 1)\n",
      "W1 = (50, 50)\n",
      "W2 = (50, 50)\n",
      "W3 = (50, 1)\n",
      "Epoch 7/20\n",
      "1000/1000 [==============================] - 0s 164us/step - loss: 27528935.1875\n",
      "on_epoch_end\n",
      "h_local1_weight = (50, 2, 1)\n",
      "h_local2_weight = (50, 1, 1)\n",
      "h0 = (50, 2)\n",
      "h0_abs = (50, 2)\n",
      "h1 = (50, 50)\n",
      "h2 = (50, 50)\n",
      "h3 = (50, 1)\n",
      "W1 = (50, 50)\n",
      "W2 = (50, 50)\n",
      "W3 = (50, 1)\n",
      "Epoch 8/20\n",
      "1000/1000 [==============================] - 0s 164us/step - loss: 26209058.6713\n",
      "on_epoch_end\n",
      "h_local1_weight = (50, 2, 1)\n",
      "h_local2_weight = (50, 1, 1)\n",
      "h0 = (50, 2)\n",
      "h0_abs = (50, 2)\n",
      "h1 = (50, 50)\n",
      "h2 = (50, 50)\n",
      "h3 = (50, 1)\n",
      "W1 = (50, 50)\n",
      "W2 = (50, 50)\n",
      "W3 = (50, 1)\n",
      "Epoch 9/20\n",
      "1000/1000 [==============================] - 0s 163us/step - loss: 24485695.6000\n",
      "on_epoch_end\n",
      "h_local1_weight = (50, 2, 1)\n",
      "h_local2_weight = (50, 1, 1)\n",
      "h0 = (50, 2)\n",
      "h0_abs = (50, 2)\n",
      "h1 = (50, 50)\n",
      "h2 = (50, 50)\n",
      "h3 = (50, 1)\n",
      "W1 = (50, 50)\n",
      "W2 = (50, 50)\n",
      "W3 = (50, 1)\n",
      "Epoch 10/20\n",
      "1000/1000 [==============================] - 0s 164us/step - loss: 22328479.5550\n",
      "on_epoch_end\n",
      "h_local1_weight = (50, 2, 1)\n",
      "h_local2_weight = (50, 1, 1)\n",
      "h0 = (50, 2)\n",
      "h0_abs = (50, 2)\n",
      "h1 = (50, 50)\n",
      "h2 = (50, 50)\n",
      "h3 = (50, 1)\n",
      "W1 = (50, 50)\n",
      "W2 = (50, 50)\n",
      "W3 = (50, 1)\n",
      "Epoch 11/20\n",
      "1000/1000 [==============================] - 0s 165us/step - loss: 20006874.4575\n",
      "on_epoch_end\n",
      "h_local1_weight = (50, 2, 1)\n",
      "h_local2_weight = (50, 1, 1)\n",
      "h0 = (50, 2)\n",
      "h0_abs = (50, 2)\n",
      "h1 = (50, 50)\n",
      "h2 = (50, 50)\n",
      "h3 = (50, 1)\n",
      "W1 = (50, 50)\n",
      "W2 = (50, 50)\n",
      "W3 = (50, 1)\n",
      "Epoch 12/20\n",
      "1000/1000 [==============================] - 0s 138us/step - loss: 17930938.5300\n",
      "on_epoch_end\n",
      "h_local1_weight = (50, 2, 1)\n",
      "h_local2_weight = (50, 1, 1)\n",
      "h0 = (50, 2)\n",
      "h0_abs = (50, 2)\n",
      "h1 = (50, 50)\n",
      "h2 = (50, 50)\n",
      "h3 = (50, 1)\n",
      "W1 = (50, 50)\n",
      "W2 = (50, 50)\n",
      "W3 = (50, 1)\n",
      "Epoch 13/20\n",
      "1000/1000 [==============================] - 0s 121us/step - loss: 16309222.4250\n",
      "on_epoch_end\n",
      "h_local1_weight = (50, 2, 1)\n",
      "h_local2_weight = (50, 1, 1)\n",
      "h0 = (50, 2)\n",
      "h0_abs = (50, 2)\n",
      "h1 = (50, 50)\n",
      "h2 = (50, 50)\n",
      "h3 = (50, 1)\n",
      "W1 = (50, 50)\n",
      "W2 = (50, 50)\n",
      "W3 = (50, 1)\n",
      "Epoch 14/20\n",
      "1000/1000 [==============================] - 0s 119us/step - loss: 15078601.3163\n",
      "on_epoch_end\n",
      "h_local1_weight = (50, 2, 1)\n",
      "h_local2_weight = (50, 1, 1)\n",
      "h0 = (50, 2)\n",
      "h0_abs = (50, 2)\n",
      "h1 = (50, 50)\n",
      "h2 = (50, 50)\n",
      "h3 = (50, 1)\n",
      "W1 = (50, 50)\n",
      "W2 = (50, 50)\n",
      "W3 = (50, 1)\n",
      "Epoch 15/20\n",
      "1000/1000 [==============================] - 0s 119us/step - loss: 14074024.5800\n",
      "on_epoch_end\n",
      "h_local1_weight = (50, 2, 1)\n",
      "h_local2_weight = (50, 1, 1)\n",
      "h0 = (50, 2)\n",
      "h0_abs = (50, 2)\n",
      "h1 = (50, 50)\n",
      "h2 = (50, 50)\n",
      "h3 = (50, 1)\n",
      "W1 = (50, 50)\n",
      "W2 = (50, 50)\n",
      "W3 = (50, 1)\n",
      "Epoch 16/20\n",
      "1000/1000 [==============================] - 0s 137us/step - loss: 13194063.9275\n",
      "on_epoch_end\n",
      "h_local1_weight = (50, 2, 1)\n",
      "h_local2_weight = (50, 1, 1)\n",
      "h0 = (50, 2)\n",
      "h0_abs = (50, 2)\n",
      "h1 = (50, 50)\n",
      "h2 = (50, 50)\n",
      "h3 = (50, 1)\n",
      "W1 = (50, 50)\n",
      "W2 = (50, 50)\n",
      "W3 = (50, 1)\n",
      "Epoch 17/20\n",
      "1000/1000 [==============================] - 0s 144us/step - loss: 12415991.8275\n",
      "on_epoch_end\n",
      "h_local1_weight = (50, 2, 1)\n",
      "h_local2_weight = (50, 1, 1)\n",
      "h0 = (50, 2)\n",
      "h0_abs = (50, 2)\n",
      "h1 = (50, 50)\n",
      "h2 = (50, 50)\n",
      "h3 = (50, 1)\n",
      "W1 = (50, 50)\n",
      "W2 = (50, 50)\n",
      "W3 = (50, 1)\n",
      "Epoch 18/20\n",
      "1000/1000 [==============================] - 0s 140us/step - loss: 11823664.3650\n",
      "on_epoch_end\n",
      "h_local1_weight = (50, 2, 1)\n",
      "h_local2_weight = (50, 1, 1)\n",
      "h0 = (50, 2)\n",
      "h0_abs = (50, 2)\n",
      "h1 = (50, 50)\n",
      "h2 = (50, 50)\n",
      "h3 = (50, 1)\n",
      "W1 = (50, 50)\n",
      "W2 = (50, 50)\n",
      "W3 = (50, 1)\n",
      "Epoch 19/20\n",
      "1000/1000 [==============================] - 0s 133us/step - loss: 11324550.7625\n",
      "on_epoch_end\n",
      "h_local1_weight = (50, 2, 1)\n",
      "h_local2_weight = (50, 1, 1)\n",
      "h0 = (50, 2)\n",
      "h0_abs = (50, 2)\n",
      "h1 = (50, 50)\n",
      "h2 = (50, 50)\n",
      "h3 = (50, 1)\n",
      "W1 = (50, 50)\n",
      "W2 = (50, 50)\n",
      "W3 = (50, 1)\n",
      "Epoch 20/20\n",
      "1000/1000 [==============================] - 0s 134us/step - loss: 11033440.1525\n",
      "on_epoch_end\n",
      "h_local1_weight = (50, 2, 1)\n",
      "h_local2_weight = (50, 1, 1)\n",
      "h0 = (50, 2)\n",
      "h0_abs = (50, 2)\n",
      "h1 = (50, 50)\n",
      "h2 = (50, 50)\n",
      "h3 = (50, 1)\n",
      "W1 = (50, 50)\n",
      "W2 = (50, 50)\n",
      "W3 = (50, 1)\n"
     ]
    }
   ],
   "source": [
    "''''''\n",
    "# After generating the knockoff data, run DNN\n",
    "file_name = 'X_n1000_p50_rep20_chol_lu_knockoff.csv';\n",
    "X_knockoff_data = pd.read_csv(file_name);\n",
    "# X_knockoff_data\n",
    "\n",
    "#nutrient_data\n",
    "original_data_Y = pd.read_csv('y_si_n1000_p50_rep20.csv');\n",
    "# original_data_Y\n",
    "\n",
    "X_values = X_knockoff_data.values;\n",
    "Y_values = original_data_Y.values;\n",
    "    \n",
    "pVal = int(X_values.shape[1] / 2);\n",
    "n = X_values.shape[0];\n",
    "print(X_values.shape);\n",
    "print(Y_values.shape);\n",
    "print(pVal);\n",
    "    \n",
    "X_origin = X_values[:, 0:pVal];\n",
    "X_knockoff = X_values[:, pVal:];\n",
    "\n",
    "x3D_train = np.zeros((n, pVal, 2));\n",
    "x3D_train[:, :, 0] = X_origin;\n",
    "x3D_train[:, :, 1] = X_knockoff;\n",
    "label_train = Y_values;\n",
    "    \n",
    "coeff = 0.05 * np.sqrt(2.0 * np.log(pVal) / n);\n",
    "\n",
    "n_outputs = original_data_Y.shape[1];\n",
    "\n",
    "#Save the DNN output to the following directory.\n",
    "result_dir = 'result/';\n",
    "if not os.path.exists(result_dir):\n",
    "    os.makedirs(result_dir);\n",
    "    \n",
    "from DL.DNN.DNN import DNN;\n",
    "dnn = DNN();\n",
    "model = dnn.build_DNN(pVal, n_outputs, coeff);\n",
    "callback = DNN.Job_finish_Callback(result_dir,pVal);\n",
    "dnn.train_DNN(model, x3D_train, label_train,callback);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__init__\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "#Apply FDR control to DNN result\n",
    "from DL.FDR.FDR_control import FDR_control;\n",
    "control = FDR_control();\n",
    "selected_features = control.controlFilter(\"/ihome/hpark/zhf16/causalDeepVASE/X_n1000_p50_rep20.csv\", \"/ihome/hpark/zhf16/causalDeepVASE/result\", offset=1, q=0.05);\n",
    "#Save the selected associations\n",
    "selected_associations = [];\n",
    "for ele in selected_features:\n",
    "    selected_associations.append({\"Feature1\":ele,\"Feature2\":\"Y\"});\n",
    "pd.DataFrame(selected_associations).to_csv(\"DNN_selected_associations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 51)\n",
      "(1000, 51)\n"
     ]
    }
   ],
   "source": [
    "#Run DG\n",
    "#Load data\n",
    "X_data = pd.read_csv(\"X_n1000_p50_rep20.csv\");\n",
    "# X_data\n",
    "Y_data = pd.read_csv('y_si_n1000_p50_rep20.csv');\n",
    "#Merge X and Y\n",
    "dataset = pd.concat([X_data, Y_data], axis=1, join='inner');\n",
    "print(dataset.shape);\n",
    "\n",
    "#Calculate the covariance matrix\n",
    "cov_mat = dataset.cov();\n",
    "corr_inv = np.linalg.inv(cov_mat)\n",
    "corr_inv = pd.DataFrame(data=corr_inv, index=cov_mat.index,columns=cov_mat.columns)\n",
    "# corr_inv.head(2)\n",
    "\n",
    "#Convert the columns to their numerical representations\n",
    "col_map = {};\n",
    "col_map_rev = {};\n",
    "col_list = dataset.columns.to_list();\n",
    "for index,ele in enumerate(col_list):\n",
    "    col_map[ele] = index;\n",
    "    col_map_rev[index] = ele;\n",
    "print(dataset.shape);\n",
    "\n",
    "#https://stats.stackexchange.com/questions/13810/threshold-for-correlation-coefficient-to-indicate-statistical-significance-of-a\n",
    "# t = dataset.shape[0]**(1/2)\n",
    "\n",
    "#The data may need to be normalized if neccessary.\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# scaler = MinMaxScaler();\n",
    "# scaled_values = scaler.fit_transform(dataset);\n",
    "# dataset.loc[:,:] = scaled_values;\n",
    "\n",
    "#Initialize DG object\n",
    "from causal.DegenerateGaussianScore import DegenerateGaussianScore\n",
    "dg = DegenerateGaussianScore(dataset,discrete_threshold=0.2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found.\n",
      "Found.\n",
      "Found.\n",
      "Found.\n"
     ]
    }
   ],
   "source": [
    "selected_associations_sum = [];\n",
    "#Load both MGM-identified and DNN associations\n",
    "MGM_associations = pd.read_csv(\"X_n1000_p50_rep20_MGM_associations.csv\");\n",
    "for index,row in MGM_associations.iterrows():\n",
    "    if row[\"Feature1\"]==\"Y\" or row[\"Feature2\"]==\"Y\":\n",
    "        print(\"Found.\");\n",
    "        selected_associations_sum.append({\"Feature1\":row[\"Feature1\"],\"Feature2\":row[\"Feature2\"]});\n",
    "        \n",
    "DNN_associations = pd.read_csv(\"DNN_selected_associations.csv\");\n",
    "for index,row in DNN_associations.iterrows():\n",
    "    selected_associations_sum.append({\"Feature1\":row[\"Feature1\"],\"Feature2\":row[\"Feature2\"]});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cause: F1, Effect: Y\n",
      "Cause: F2, Effect: Y\n",
      "Cause: F3, Effect: Y\n",
      "Cause: F4, Effect: Y\n",
      "Cause: F1, Effect: Y\n",
      "Cause: F2, Effect: Y\n",
      "Cause: F3, Effect: Y\n",
      "Cause: F4, Effect: Y\n",
      "Cause: F5, Effect: Y\n",
      "Cause: F6, Effect: Y\n",
      "Cause: F7, Effect: Y\n",
      "Cause: F8, Effect: Y\n",
      "Cause: F9, Effect: Y\n",
      "Cause: F10, Effect: Y\n",
      "Cause: F11, Effect: Y\n",
      "Cause: F12, Effect: Y\n",
      "Cause: F13, Effect: Y\n",
      "Cause: F14, Effect: Y\n",
      "Cause: F15, Effect: Y\n",
      "Cause: F16, Effect: Y\n",
      "Cause: F19, Effect: Y\n",
      "Cause: F22, Effect: Y\n",
      "Cause: F23, Effect: Y\n",
      "Cause: F24, Effect: Y\n",
      "Cause: F27, Effect: Y\n",
      "Cause: F29, Effect: Y\n",
      "Cause: F30, Effect: Y\n",
      "Cause: F31, Effect: Y\n",
      "Cause: F33, Effect: Y\n",
      "Cause: F34, Effect: Y\n",
      "Cause: F37, Effect: Y\n",
      "Cause: F38, Effect: Y\n",
      "Cause: F40, Effect: Y\n",
      "Cause: F43, Effect: Y\n",
      "Cause: F44, Effect: Y\n",
      "Cause: F47, Effect: Y\n",
      "Cause: F49, Effect: Y\n",
      "Cause: F50, Effect: Y\n"
     ]
    }
   ],
   "source": [
    "for ele in selected_associations_sum:\n",
    "    f1 = ele[\"Feature1\"];\n",
    "    f2 = ele[\"Feature2\"];\n",
    "    \n",
    "    inv_val = abs(corr_inv[f1][f2]);\n",
    "    if inv_val<0.0:\n",
    "        continue;\n",
    "    \n",
    "    n1_idx = col_map[f1];\n",
    "    n2_idx = col_map[f2];\n",
    "    \n",
    "    s1 = dg.localScore(n1_idx,{n2_idx});\n",
    "    s2 = dg.localScore(n2_idx,{n1_idx});\n",
    "    \n",
    "    if s1<s2:\n",
    "        print(\"Cause: \"+f2+\", Effect: \"+f1);\n",
    "    elif s1>s2:\n",
    "        print(\"Cause: \"+f1+\", Effect: \"+f2);\n",
    "    else:\n",
    "        print(\"Same score.\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env36",
   "language": "python",
   "name": "env36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
